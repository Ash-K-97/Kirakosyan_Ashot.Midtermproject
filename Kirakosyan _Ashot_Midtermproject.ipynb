{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb054fa6-2b75-4c4b-a125-c427340ed28d",
   "metadata": {},
   "source": [
    "# Midterm Project (Cs634)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19777e14-c189-4e8c-bb6c-1835db29f6f1",
   "metadata": {},
   "source": [
    "Name - Ashot Kirakosyan<br>\n",
    "NJIT ID - ak2095<br>\n",
    "Email - ak2995@njit.edu<br>\n",
    "Date: 10/13/2024<br>\n",
    "Professor - Yasser Abduallah"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cf53c7-5317-4823-9032-50e057437195",
   "metadata": {},
   "source": [
    "### Abstract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac4e846-3d06-4692-b34b-6391f635a357",
   "metadata": {},
   "source": [
    "This report discusses a Python implementation for performing association rule mining using three different algorithms: Brute Force, Apriori, and FP-Growth. The goal is to analyze transactions from selected datasets to identify frequently purchased itemsets and generate association rules based on user-defined parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c8a060-8c2a-4a5c-9ddc-2c8b2592f8a7",
   "metadata": {},
   "source": [
    "### Dataset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82f838a-93cd-487f-bed8-c4289fc59d53",
   "metadata": {},
   "source": [
    "The available datasets are:<br>\n",
    "1. Amazon (Created by using an example in Canvas)<br>\n",
    "2. Farmmarket(Created by using ChatGPT experimental!!!)<br>\n",
    "3. Wholefoods (The big data containing 1000 transactions and a total of 20 items. Downloaded from Github user name luoyetx https://github.com/luoyetx/Apriori/blob/master/data.csv)<br>\n",
    "4. Bestbuy (Created by using an example in Canvas) <br>\n",
    "5. Kmart (Created by using an example in Canvas)<br>\n",
    "The user can select a dataset and input two critical parameters: minimum support and minimum confidence. These parameters help determine the significance of the itemsets and the strength of the association rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1af592-b167-4f1b-8af0-d9d6b1c9a1a3",
   "metadata": {},
   "source": [
    "### User Input Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c9adbd-f138-4d89-aa94-bcfad939e5e2",
   "metadata": {},
   "source": [
    "The function select_store_and_input_parameters allows users to:<br>\n",
    "1.Select a dataset.<br>\n",
    "2.Confirm their selection.<br>\n",
    "3.Input the minimum support and confidence, with validation checks to ensure values are within acceptable ranges (0 to 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1f75e8-5f71-4b73-9952-aff4b462819a",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e746360-fa74-4b2b-b4f3-66dd81783810",
   "metadata": {},
   "source": [
    "Once the dataset is selected, the corresponding CSV files are read into Pandas DataFrames. The transactions are preprocessed to:<br>\n",
    "1. Remove unnecessary characters (like \\x92).<br>\n",
    "2. Ensure items are in a consistent string format and sorted according to a predefined order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330c02c7-6b2e-49d9-ad92-571c948570d1",
   "metadata": {},
   "source": [
    "### Frequent Itemset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2e04f5-1b38-451c-a3ee-f4ffbd27efff",
   "metadata": {},
   "source": [
    "The implementation provides two approaches to generate frequent itemsets:<br>\n",
    "\n",
    "1. Brute Force Approach: This method counts all possible itemsets generated from transactions. It filters itemsets based on the minimum support to identify frequent itemsets.<br>\n",
    "2. Apriori Algorithm: Using the apriori_python library, the algorithm efficiently generates frequent itemsets by reducing the search space through the property of support.<br>\n",
    "3. FP-Growth Algorithm: Using the pyfpgrowth library, this algorithm constructs a frequent pattern tree (FP-tree) to discover frequent itemsets without candidate generation, making it faster for larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa9c902-5df4-4e4e-8f59-a8dcca55a081",
   "metadata": {},
   "source": [
    "### Association Rule Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae5d43-183b-49fc-8984-a7715d24a49c",
   "metadata": {},
   "source": [
    "For each frequent itemset, association rules are generated based on the specified minimum confidence. The following process is followed:<BR>\n",
    "1. For each frequent itemset, possible antecedents are determined, and the consequent is derived.<br>\n",
    "2. The confidence for each rule is calculated as the ratio of the support of the itemset to the support of the antecedent.<br>\n",
    "3. Rules that meet or exceed the minimum confidence are retained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab66693f-d579-4caa-b2eb-019c7da51916",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9621db7e-09e3-4ed7-ada6-1687dc997a3c",
   "metadata": {},
   "source": [
    "1. Brute Force Frequent Itemsets: The results list frequent itemsets along with their counts and support values.<br>\n",
    "2. Brute Force Association Rules: The rules are displayed in the format of antecedent -> consequent, showing their support and confidence. <br>\n",
    "3. Apriori Frequent Itemsets and Rules: A similar output is generated using the Apriori algorithm, providing an alternative view of the relationships between items. <br>\n",
    "4. FP-Growth Frequent Itemsets and Rules: The results from the FP-Growth algorithm are printed, showcasing its efficiency and effectiveness in mining associations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97698795-4af3-4656-afe9-4f3042988bef",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d47decc-88dd-4527-9a6f-b4f5220914ad",
   "metadata": {},
   "source": [
    "To run this program, the following software is required:\n",
    "Python: Version 3.6 or higher.<br>\n",
    "Libraries:<br>\n",
    "1. pandas<br>\n",
    "2. numpy <Br>\n",
    "3. pyfpgrowth <br>\n",
    "4. apriori_python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea12dd6a-ead9-45d3-949b-ba091125280d",
   "metadata": {},
   "source": [
    "### Installation Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010d269d-ad1b-4e71-bd58-837ff3de9fa6",
   "metadata": {},
   "source": [
    "Install Python: Download and install Python from the official website. https://www.python.org/downloads/ <br>\n",
    "1. Open Command Line Interface (CLI):<br>\n",
    " .Windows: Search for \"cmd\" in the Start menu. <br>\n",
    " . macOS/Linux: Open the Terminal application. <br>\n",
    "2. Install Required Libraries: Use the following command to install the necessary libraries:<br>\n",
    "    pip install pandas numpy pyfpgrowth apriori_python\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26765f2e-5778-4ecb-8aa4-6457e6b4e565",
   "metadata": {},
   "source": [
    "### Download repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82acc8b7-56d5-439b-86bb-a4e2bdb197c3",
   "metadata": {},
   "source": [
    "Link to the repository https://github.com/Ash-K-97/Kirakosyan_Ashot.Midtermproject<br>\n",
    "Download the zip file and extract all files into one folder<BR>\n",
    "Read a readme file and follow the instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4656d72-4011-4c32-82c1-9ee528f76079",
   "metadata": {},
   "source": [
    "### How to Run the Program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44fe08d-4c85-4b37-92bb-e6a693932649",
   "metadata": {},
   "source": [
    "1. After downloading the repository and extracting the files move it to the directory of your choice<br>\n",
    "2. Run the Program: In the CLI, navigate to the directory containing the script and execute:<br>\n",
    "    .Example: cd C:\\Users\\YourName\\Documents\\Kirakosyan_Ashot.Midtermproject<br>\n",
    "    .where: YourName is the name of the user.<br>\n",
    "3. After code execution, you should be in the directory of the file <br>\n",
    "4. You can check which Python files are in this directory by using the following command: dir <br>\n",
    "5. Once you see the Python file you want to run, you can execute it by typing: python Midtermproject_code.py<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d162cb9d-96f4-46ea-8624-6c64586cd317",
   "metadata": {},
   "source": [
    "### Program Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cee887-0f3d-4276-ba3c-c7e07ebaf53a",
   "metadata": {},
   "source": [
    "1. Store Selection: The program prompts the user to select a store from a list.<br>\n",
    "2. Parameter Input: Users are prompted to input the minimum support and confidence values.<br>\n",
    "3. Data Loading: The program reads the selected store's transaction and item list CSV files.<br>\n",
    "4. Algorithm Execution:<br>\n",
    "    .Brute Force: Computes frequent itemsets and generates association rules.<br>\n",
    "    .Apriori: Uses the Apriori algorithm to find frequent itemsets and rules.<br>\n",
    "    .FP-Growth: Utilizes the FP-Growth algorithm for the same purpose.<br>\n",
    "5. Output Results: The program displays frequent itemsets and association rules for each algorithm, along with their execution times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1238590-2529-4e75-9b7d-971466805205",
   "metadata": {},
   "source": [
    "### Below is the running code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f85b32c-5d86-43c9-955e-88cabefcbef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, welcome to my Midterm project\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter the index of the store you want to check (0 for Amazon, 1 for Farmmarket, 2 for Wholefood, 3 for Bestbuy, 4 for Kmart):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You selected store: Wholefood\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Is this correct? (yes/no):  yes\n",
      "Enter minimum support (as a decimal between 0 and 1):  0.3\n",
      "Enter minimum confidence (as a decimal between 0 and 1):  0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceeding with store: Wholefood\n",
      "Minimum support: 0.3, Minimum confidence: 0.3\n",
      "data_Wholefood.csv\n",
      "\n",
      "Brute Force Frequent Itemsets:\n",
      "Itemset: {'apples'}, Count: 314, Support: 0.3137\n",
      "Itemset: {'bourbon'}, Count: 403, Support: 0.4026\n",
      "Itemset: {'chicken'}, Count: 315, Support: 0.3147\n",
      "Itemset: {'corned_b'}, Count: 391, Support: 0.3906\n",
      "Itemset: {'cracker'}, Count: 488, Support: 0.4875\n",
      "Itemset: {'baguette'}, Count: 392, Support: 0.3916\n",
      "Itemset: {'ham'}, Count: 305, Support: 0.3047\n",
      "Itemset: {'ice_crea'}, Count: 313, Support: 0.3127\n",
      "Itemset: {'olives'}, Count: 473, Support: 0.4725\n",
      "Itemset: {'hering'}, Count: 486, Support: 0.4855\n",
      "Itemset: {'avocado'}, Count: 363, Support: 0.3626\n",
      "Itemset: {'heineken'}, Count: 600, Support: 0.5994\n",
      "Itemset: {'soda'}, Count: 318, Support: 0.3177\n",
      "Itemset: {'cracker', 'heineken'}, Count: 366, Support: 0.3656\n",
      "Itemset: {'artichok'}, Count: 305, Support: 0.3047\n",
      "\n",
      "Brute Force Association Rules (Antecedent -> Consequent):\n",
      "{'cracker'} -> {'heineken'}, Support: 0.3656, Confidence: 0.7500\n",
      "{'heineken'} -> {'cracker'}, Support: 0.3656, Confidence: 0.6100\n",
      "\n",
      "Apriori Frequent Itemsets:\n",
      "Itemset: {'chicken'}, Support: 1\n",
      "Itemset: {'baguette'}, Support: 1\n",
      "Itemset: {'ice_crea'}, Support: 1\n",
      "Itemset: {'corned_b'}, Support: 1\n",
      "Itemset: {'artichok'}, Support: 1\n",
      "Itemset: {'apples'}, Support: 1\n",
      "Itemset: {'heineken'}, Support: 1\n",
      "Itemset: {'soda'}, Support: 1\n",
      "Itemset: {'avocado'}, Support: 1\n",
      "Itemset: {'ham'}, Support: 1\n",
      "Itemset: {'bourbon'}, Support: 1\n",
      "Itemset: {'hering'}, Support: 1\n",
      "Itemset: {'olives'}, Support: 1\n",
      "Itemset: {'cracker'}, Support: 1\n",
      "Itemset: {'cracker', 'heineken'}, Support: 2\n",
      "\n",
      "Apriori Association Rules using apriori-python (Antecedent -> Consequent):\n",
      "Rule 1: {'heineken'} -> {'cracker'}, Confidence: 0.6100\n",
      "Rule 2: {'cracker'} -> {'heineken'}, Confidence: 0.7500\n",
      "\n",
      "FP-Growth Frequent Itemsets:\n",
      "Itemset: {'ham'}, Count: 305\n",
      "Itemset: {'artichok'}, Count: 305\n",
      "Itemset: {'ice_crea'}, Count: 313\n",
      "Itemset: {'apples'}, Count: 314\n",
      "Itemset: {'chicken'}, Count: 315\n",
      "Itemset: {'soda'}, Count: 318\n",
      "Itemset: {'avocado'}, Count: 363\n",
      "Itemset: {'corned_b'}, Count: 391\n",
      "Itemset: {'baguette'}, Count: 392\n",
      "Itemset: {'bourbon'}, Count: 403\n",
      "Itemset: {'olives'}, Count: 473\n",
      "Itemset: {'hering'}, Count: 486\n",
      "Itemset: {'cracker'}, Count: 488\n",
      "Itemset: {'cracker', 'heineken'}, Count: 366\n",
      "Itemset: {'heineken'}, Count: 600\n",
      "\n",
      "FP-Growth Association Rules (Antecedent -> Consequent):\n",
      "{'cracker'} -> {'heineken'}, Confidence: 0.7500\n",
      "{'heineken'} -> {'cracker'}, Confidence: 0.6100\n",
      "\n",
      "Performance Summary:\n",
      "Brute Force Execution Time: 0.0290 seconds\n",
      "Apriori Execution Time: 0.0265 seconds\n",
      "FP-Growth Execution Time: 0.0088 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pyfpgrowth\n",
    "from apriori_python.apriori import apriori\n",
    "import time  \n",
    "print(\"Hello, welcome to my Midterm project\")\n",
    "# Create Dataset\n",
    "datasetlist = ['Amazon', 'Farmmarket','Wholefood','Bestbuy','Kmart']\n",
    "\n",
    "# Function to confirm the selected store and input min_support and min_confidence\n",
    "def select_store_and_input_parameters():\n",
    "    while True:\n",
    "        try:\n",
    "            selected_file = int(input(\"Please enter the index of the store you want to check (0 for Amazon, 1 for Farmmarket, 2 for Wholefood, 3 for Bestbuy, 4 for Kmart): \"))\n",
    "            if selected_file < 0 or selected_file >= len(datasetlist):\n",
    "                print(\"Invalid selection. Please select a valid store.\")\n",
    "                continue\n",
    "            print(f\"You selected store: {datasetlist[selected_file]}\")\n",
    "            confirmation = input(\"Is this correct? (yes/no): \").strip().lower()\n",
    "            if confirmation == 'yes':\n",
    "                min_support = float(input(\"Enter minimum support (as a decimal between 0 and 1): \"))\n",
    "                if not (0 <= min_support <= 1):\n",
    "                    raise ValueError(\"Minimum support must be between 0 and 1.\")\n",
    "                min_confidence = float(input(\"Enter minimum confidence (as a decimal between 0 and 1): \"))\n",
    "                if not (0 <= min_confidence <= 1):\n",
    "                    raise ValueError(\"Minimum confidence must be between 0 and 1.\")\n",
    "                return selected_file, min_support, min_confidence\n",
    "            elif confirmation == 'no':\n",
    "                print(\"Returning to store selection...\\n\")\n",
    "                continue\n",
    "            else:\n",
    "                print(\"Invalid input. Please type 'yes' or 'no'.\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Input Error: {e}\")\n",
    "            continue\n",
    "\n",
    "# Call the function to get user input for store selection, min_support, and min_confidence\n",
    "selected_file, min_support, min_confidence = select_store_and_input_parameters()\n",
    "\n",
    "print(f\"Proceeding with store: {datasetlist[selected_file]}\")\n",
    "print(f\"Minimum support: {min_support}, Minimum confidence: {min_confidence}\")\n",
    "\n",
    "# Open and read the corresponding CSV file\n",
    "file_name = 'data_' + datasetlist[selected_file] + '.csv'\n",
    "list_name = 'datalist_' + datasetlist[selected_file] + '.csv'\n",
    "print(file_name)\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "df = pd.read_csv(file_name, encoding='ISO-8859-1')\n",
    "df_list = pd.read_csv(list_name, encoding='ISO-8859-1')\n",
    "\n",
    "# Prepare the order list and dataset\n",
    "order = sorted(df_list['Item.name'].astype(str))\n",
    "dataset = []\n",
    "\n",
    "for lines in df['Transaction']:\n",
    "    trans = [str(item.strip().replace('\\x92', \"'\")) for item in lines.strip().split(',')]\n",
    "    trans_1 = sorted(np.unique(trans), key=lambda x: order.index(x) if x in order else float('inf'))\n",
    "    dataset.append(trans_1)\n",
    "\n",
    "# Brute Force: Function to get frequent itemsets using brute force\n",
    "def get_frequent_itemsets(dataset, min_support):\n",
    "    itemset_counts = defaultdict(int)\n",
    "    num_transactions = len(dataset)\n",
    "\n",
    "    for transaction in dataset:\n",
    "        for k in range(1, len(transaction) + 1):  \n",
    "            for itemset in itertools.combinations(transaction, k):\n",
    "                itemset_counts[itemset] += 1\n",
    "\n",
    "    frequent_itemsets = {itemset: count for itemset, count in itemset_counts.items() if count / num_transactions >= min_support}\n",
    "    return frequent_itemsets\n",
    "\n",
    "# Function to generate association rules from frequent itemsets\n",
    "def generate_association_rules(frequent_itemsets, dataset, min_confidence):\n",
    "    rules = []\n",
    "    num_transactions = len(dataset)\n",
    "\n",
    "    for itemset, count in frequent_itemsets.items():\n",
    "        for k in range(1, len(itemset)):\n",
    "            for antecedent in itertools.combinations(itemset, k):\n",
    "                antecedent = set(antecedent)\n",
    "                consequent = set(itemset) - antecedent\n",
    "\n",
    "                if len(consequent) > 0:\n",
    "                    antecedent_count = sum(1 for transaction in dataset if antecedent.issubset(set(transaction)))\n",
    "                    rule_support = count\n",
    "                    rule_confidence = rule_support / antecedent_count if antecedent_count > 0 else 0\n",
    "\n",
    "                    if rule_confidence >= min_confidence:\n",
    "                        rules.append((antecedent, consequent, rule_support, rule_confidence))\n",
    "\n",
    "    return rules\n",
    "\n",
    "# Measure the performance of Brute Force and execute the function.\n",
    "start_time = time.time()\n",
    "frequent_itemsets = get_frequent_itemsets(dataset, min_support)\n",
    "brute_force_rules = generate_association_rules(frequent_itemsets, dataset, min_confidence)\n",
    "\n",
    "\n",
    "print(\"\\nBrute Force Frequent Itemsets:\")\n",
    "for itemset, count in frequent_itemsets.items():\n",
    "    print(f\"Itemset: {set(map(str, itemset))}, Count: {count}, Support: {count / len(dataset):.4f}\")\n",
    "\n",
    "print(\"\\nBrute Force Association Rules (Antecedent -> Consequent):\")\n",
    "for antecedent, consequent, support, confidence in brute_force_rules:\n",
    "    antecedent_str = set(map(str, antecedent))\n",
    "    consequent_str = set(map(str, consequent))\n",
    "    print(f\"{antecedent_str} -> {consequent_str}, Support: {support / len(dataset):.4f}, Confidence: {confidence:.4f}\")\n",
    "brute_force_time = time.time() - start_time\n",
    "# Measure the performance of Apriori algorithm and execute the function.\n",
    "start_time = time.time()\n",
    "frequent_itemsets_apriori, apriori_rules = apriori(dataset, minSup=min_support, minConf=min_confidence)\n",
    "\n",
    "print(\"\\nApriori Frequent Itemsets:\")\n",
    "for support, itemsets in frequent_itemsets_apriori.items():\n",
    "    for itemset in itemsets:\n",
    "        itemset_str = set(map(str, itemset))\n",
    "        print(f\"Itemset: {itemset_str}, Support: {support}\")\n",
    "\n",
    "if isinstance(apriori_rules, list) and len(apriori_rules) > 0:\n",
    "    print(\"\\nApriori Association Rules using apriori-python (Antecedent -> Consequent):\")\n",
    "    for i, rule in enumerate(apriori_rules):\n",
    "        antecedent = set(map(str, rule[0]))\n",
    "        consequent = set(map(str, rule[1]))\n",
    "        confidence = rule[2]\n",
    "        print(f\"Rule {i + 1}: {antecedent} -> {consequent}, Confidence: {confidence:.4f}\")\n",
    "apriori_time = time.time() - start_time\n",
    "# Measure the performance of FP-Growth and execute the function.\n",
    "start_time = time.time()\n",
    "def run_fp_growth(dataset, min_support):\n",
    "    min_support_count = int(min_support * len(dataset))\n",
    "    patterns = pyfpgrowth.find_frequent_patterns(dataset, min_support_count)\n",
    "    rules = pyfpgrowth.generate_association_rules(patterns, min_confidence)\n",
    "    return patterns, rules\n",
    "\n",
    "\n",
    "patterns_fp_growth, rules_fp_growth = run_fp_growth(dataset, min_support)\n",
    "\n",
    "print(\"\\nFP-Growth Frequent Itemsets:\")\n",
    "for pattern, count in patterns_fp_growth.items():\n",
    "    pattern_str = set(map(str, pattern))\n",
    "    print(f\"Itemset: {pattern_str}, Count: {count}\")\n",
    "\n",
    "print(\"\\nFP-Growth Association Rules (Antecedent -> Consequent):\")\n",
    "for antecedent, (consequent, confidence) in rules_fp_growth.items():\n",
    "    antecedent_str = set(map(str, antecedent))\n",
    "    consequent_str = set(map(str, consequent))\n",
    "    print(f\"{antecedent_str} -> {consequent_str}, Confidence: {confidence:.4f}\")\n",
    "fp_growth_time = time.time() - start_time\n",
    "\n",
    "# Performance Summary\n",
    "print(\"\\nPerformance Summary:\")\n",
    "print(f\"Brute Force Execution Time: {brute_force_time:.4f} seconds\")\n",
    "print(f\"Apriori Execution Time: {apriori_time:.4f} seconds\")\n",
    "print(f\"FP-Growth Execution Time: {fp_growth_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1684084-e818-4e75-a114-8ffb3f7d109f",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08359ac7-ca6c-470e-8426-80a8a3f53f0b",
   "metadata": {},
   "source": [
    "The report demonstrates a comprehensive implementation of association rule mining in Python, leveraging three distinct algorithms. Each method provides unique benefits and is suitable for different scenarios depending on the dataset size and complexity. The ability to select datasets and customize parameters adds flexibility, making the tool valuable for various analytical needs in market basket analysis and other domains."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
